{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "The NLTK module is a massive tool kit, aimed at helping you with the entire Natural Language Processing (NLP) methodology. NLTK will aid you with everything from splitting sentences from paragraphs, splitting up words, recognizing the part of speech of those words, highlighting the main subjects, and then even with helping your machine to understand what the text is all about. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the task of breaking down a sentence into its constituent words/symbols.\n",
    "\n",
    "\n",
    "### Note: \n",
    "Tokenization can be subjective. Tokenization for different applications can be different.\n",
    "For example, English books and news articles have well formed sentences where as texts from social media can have many non-standard elements including smileys, emojis, urls, ellipses etc. Each case may require a different tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple split() tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'man', 'drove', 'a', 'car', 'around', 'the', 'city', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'The man drove a car around the city .'\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split() tokenizer runs into problems with punctuations.\n",
    "+ Notice 'city.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'man', 'drove', 'a', 'car', 'around', 'the', 'city.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'The man drove a car around the city.'\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More sophisticated tokenizer from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'man', 'drove', 'a', 'car', 'around', 'the', 'city', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The man drove a car around the city.'\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['However', ',', 'Lyon', 'then', 'ended', 'talks', 'with', 'the', 'Champions', 'League', 'runners-up', 'and', 'released', 'a', 'statement', 'saying', 'they', 'were', '``', 'overjoyed', 'that', 'they', 'can', 'count', 'on', 'the', 'presence', 'of', 'their', 'captain', ',', 'who', 'will', 'be', 'a', 'significant', 'part', 'of', 'their', '2018-19', 'season', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'However, Lyon then ended talks with the Champions League runners-up and \\\n",
    "            released a statement saying they were \"overjoyed that they can count on the presence of their captain, \\\n",
    "            who will be a significant part of their 2018-19 season.\"'\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['However', ',', 'Lyon', 'then', 'ended', 'talks', 'with', 'the', 'Champions', 'League', 'runners-up', 'and', 'released', 'a', 'statement', 'saying', 'they', 'were', '``', 'overjoyed', 'that', 'they', 'can', 'count', 'on', 'the', 'presence', 'of', 'their', 'captain', ',', 'who', 'will', 'be', 'a', 'significant', 'part', 'of', 'their', '2018-19', 'season', '.', '``', 'But', 'agent', 'Jean-Pierre', 'Bernes', 'said', 'Fekir', ',', 'on', 'World', 'Cup', 'duty', 'with', 'France', ',', 'could', 'still', 'end', 'up', 'at', 'Anfield', '.', '``', 'Why', 'has', \"n't\", 'he', 'signed', '?', 'Because', 'it', 'is', 'not', 'finished', '.', 'The', 'story', 'is', 'not', 'finished', ',', \"''\", 'Bernes', 'said', '.', 'Fekir', 'came', 'on', 'for', 'the', 'final', '20', 'minutes', 'as', 'France', 'beat', 'Australia', 'in', 'their', 'World', 'Cup', 'opener', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'However, Lyon then ended talks with the Champions League runners-up and released a statement saying they were \"overjoyed that they can count on the presence of their captain, who will be a significant part of their 2018-19 season.\"But agent Jean-Pierre Bernes said Fekir, on World Cup duty with France, could still end up at Anfield. \"Why hasn\\'t he signed? Because it is not finished. The story is not finished,\" Bernes said. Fekir came on for the final 20 minutes as France beat Australia in their World Cup opener.'\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, Lyon then ended talks with the Champions League runners-up and released a statement saying they were \"overjoyed that they can count on the presence of their captain, who will be a significant part of their 2018-19 season.\n",
      "-----\n",
      "\"But agent Jean-Pierre Bernes said Fekir, on World Cup duty with France, could still end up at Anfield.\n",
      "-----\n",
      "\"Why hasn't he signed?\n",
      "-----\n",
      "Because it is not finished.\n",
      "-----\n",
      "The story is not finished,\" Bernes said.\n",
      "-----\n",
      "Fekir came on for the final 20 minutes as France beat Australia in their World Cup opener.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "sentence = 'However, Lyon then ended talks with the Champions League runners-up and released a statement saying they were \"overjoyed that they can count on the presence of their captain, who will be a significant part of their 2018-19 season.\"But agent Jean-Pierre Bernes said Fekir, on World Cup duty with France, could still end up at Anfield. \"Why hasn\\'t he signed? Because it is not finished. The story is not finished,\" Bernes said. Fekir came on for the final 20 minutes as France beat Australia in their World Cup opener.'\n",
    "for sent in sent_tokenize(sentence):\n",
    "    print(sent)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Tokenizer\n",
    "Text acquired by crawling Twitter is even more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "print(tknzr.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!', 'https://twitter.com/']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "tweet = '@remy: This is waaaaayyyy too much for you!!!!!! https://twitter.com/'\n",
    "print(tknzr.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize non-English texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour M. Adam, comment allez-vous?', \"J'espère que tout va bien.\", \"Aujourd'hui est un bon jour.\"]\n",
      "---------------\n",
      "[\"J'espère\", 'que', 'tout', 'va', 'bien', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "mytext = \"Bonjour M. Adam, comment allez-vous? J'espère que tout va bien. Aujourd'hui est un bon jour.\"\n",
    "sents = sent_tokenize(mytext,\"french\") \n",
    "print(sents)\n",
    "print('---------------')\n",
    "print(word_tokenize(sents[1], 'french'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'man')\n",
      "('man', 'drove')\n",
      "('drove', 'a')\n",
      "('a', 'car')\n",
      "('car', 'around')\n",
      "('around', 'the')\n",
      "('the', 'city')\n",
      "('city', '.')\n",
      "-----\n",
      "('The', 'man', 'drove')\n",
      "('man', 'drove', 'a')\n",
      "('drove', 'a', 'car')\n",
      "('a', 'car', 'around')\n",
      "('car', 'around', 'the')\n",
      "('around', 'the', 'city')\n",
      "('the', 'city', '.')\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The man drove a car around the city .'\n",
    "tokens = sentence.split()\n",
    "for bg in bigrams(tokens):\n",
    "    print(bg)\n",
    "print('-----')\n",
    "for tg in trigrams(tokens):\n",
    "    print(tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-grams\n",
      "-----\n",
      "('The',)\n",
      "('man',)\n",
      "('drove',)\n",
      "('a',)\n",
      "('car',)\n",
      "('around',)\n",
      "('the',)\n",
      "('city',)\n",
      "('.',)\n",
      "-------------\n",
      "2-grams\n",
      "-----\n",
      "('The', 'man')\n",
      "('man', 'drove')\n",
      "('drove', 'a')\n",
      "('a', 'car')\n",
      "('car', 'around')\n",
      "('around', 'the')\n",
      "('the', 'city')\n",
      "('city', '.')\n",
      "-------------\n",
      "3-grams\n",
      "-----\n",
      "('The', 'man', 'drove')\n",
      "('man', 'drove', 'a')\n",
      "('drove', 'a', 'car')\n",
      "('a', 'car', 'around')\n",
      "('car', 'around', 'the')\n",
      "('around', 'the', 'city')\n",
      "('the', 'city', '.')\n",
      "-------------\n",
      "4-grams\n",
      "-----\n",
      "('The', 'man', 'drove', 'a')\n",
      "('man', 'drove', 'a', 'car')\n",
      "('drove', 'a', 'car', 'around')\n",
      "('a', 'car', 'around', 'the')\n",
      "('car', 'around', 'the', 'city')\n",
      "('around', 'the', 'city', '.')\n",
      "-------------\n",
      "5-grams\n",
      "-----\n",
      "('The', 'man', 'drove', 'a', 'car')\n",
      "('man', 'drove', 'a', 'car', 'around')\n",
      "('drove', 'a', 'car', 'around', 'the')\n",
      "('a', 'car', 'around', 'the', 'city')\n",
      "('car', 'around', 'the', 'city', '.')\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    print('{}-grams'.format(i))\n",
    "    print('-----')\n",
    "    for ng in ngrams(tokens, n=i):\n",
    "        print(ng)\n",
    "    print('-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', '<s>', 'The')\n",
      "('<s>', 'The', 'man')\n",
      "('The', 'man', 'drove')\n",
      "('man', 'drove', 'a')\n",
      "('drove', 'a', 'car')\n",
      "('a', 'car', 'around')\n",
      "('car', 'around', 'the')\n",
      "('around', 'the', 'city')\n",
      "('the', 'city', '.')\n",
      "('city', '.', '</s>')\n",
      "('.', '</s>', '</s>')\n"
     ]
    }
   ],
   "source": [
    "for ng in ngrams(tokens, n=3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "    print(ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#', '#', 'a')\n",
      "-------------\n",
      "('#', '#', 'i')\n",
      "('#', 'i', 's')\n",
      "-------------\n",
      "('#', '#', 't')\n",
      "('#', 't', 'h')\n",
      "('t', 'h', 'e')\n",
      "-------------\n",
      "('#', '#', 'f')\n",
      "('#', 'f', 'i')\n",
      "('f', 'i', 'r')\n",
      "('i', 'r', 's')\n",
      "('r', 's', 't')\n",
      "-------------\n",
      "('#', '#', 'l')\n",
      "('#', 'l', 'e')\n",
      "('l', 'e', 't')\n",
      "('e', 't', 't')\n",
      "('t', 't', 'e')\n",
      "('t', 'e', 'r')\n",
      "-------------\n",
      "('#', '#', 'o')\n",
      "('#', 'o', 'f')\n",
      "-------------\n",
      "('#', '#', 't')\n",
      "('#', 't', 'h')\n",
      "('t', 'h', 'e')\n",
      "-------------\n",
      "('#', '#', 'a')\n",
      "('#', 'a', 'l')\n",
      "('a', 'l', 'p')\n",
      "('l', 'p', 'h')\n",
      "('p', 'h', 'a')\n",
      "('h', 'a', 'b')\n",
      "('a', 'b', 'e')\n",
      "('b', 'e', 't')\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for word in 'a is the first letter of the alphabet'.split():\n",
    "    for ng in ngrams(word, n=3, pad_left=True, left_pad_symbol='#'):\n",
    "        print(ng)\n",
    "    print('-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('world', 'cup'): 2, ('of', 'their'): 2, ('is', 'not'): 2, ('not', 'finished'): 2, ('bernes', 'said'): 2, ('.', '``'): 2, (',', 'lyon'): 1, ('opener', '.'): 1, ('minutes', 'as'): 1, ('finished', '.'): 1, ('finished', ','): 1, ('will', 'be'): 1, ('up', 'at'): 1, ('they', 'can'): 1, ('fekir', 'came'): 1, ('.', 'the'): 1, ('came', 'on'): 1, ('in', 'their'): 1, ('can', 'count'): 1, ('has', \"n't\"): 1, ('captain', ','): 1, ('``', 'why'): 1, ('who', 'will'): 1, ('ended', 'talks'): 1, ('.', 'fekir'): 1, ('then', 'ended'): 1, ('why', 'has'): 1, ('part', 'of'): 1, ('a', 'significant'): 1, ('duty', 'with'): 1, ('and', 'released'): 1, ('a', 'statement'): 1, ('still', 'end'): 1, ('count', 'on'): 1, ('for', 'the'): 1, ('statement', 'saying'): 1, ('beat', 'australia'): 1, ('20', 'minutes'): 1, ('``', 'overjoyed'): 1, ('lyon', 'then'): 1, ('agent', 'jean-pierre'): 1, (\"n't\", 'he'): 1, ('end', 'up'): 1, ('jean-pierre', 'bernes'): 1, ('as', 'france'): 1, ('their', 'world'): 1, ('the', 'champions'): 1, ('league', 'runners-up'): 1, ('runners-up', 'and'): 1, ('2018-19', 'season'): 1, (',', \"''\"): 1, ('overjoyed', 'that'): 1, ('presence', 'of'): 1, ('released', 'a'): 1, ('final', '20'): 1, ('could', 'still'): 1, ('that', 'they'): 1, ('said', '.'): 1, (',', 'could'): 1, ('were', '``'): 1, ('france', ','): 1, ('however', ','): 1, ('he', 'signed'): 1, ('champions', 'league'): 1, ('cup', 'duty'): 1, ('the', 'presence'): 1, ('with', 'the'): 1, ('story', 'is'): 1, ('saying', 'they'): 1, ('with', 'france'): 1, ('they', 'were'): 1, ('because', 'it'): 1, ('be', 'a'): 1, ('anfield', '.'): 1, (',', 'on'): 1, ('france', 'beat'): 1, ('it', 'is'): 1, (',', 'who'): 1, ('the', 'story'): 1, ('significant', 'part'): 1, ('at', 'anfield'): 1, ('but', 'agent'): 1, ('?', 'because'): 1, ('cup', 'opener'): 1, ('on', 'the'): 1, ('the', 'final'): 1, ('signed', '?'): 1, ('on', 'world'): 1, (\"''\", 'bernes'): 1, ('australia', 'in'): 1, ('their', 'captain'): 1, ('season', '.'): 1, ('on', 'for'): 1, ('talks', 'with'): 1, ('their', '2018-19'): 1, ('said', 'fekir'): 1, ('fekir', ','): 1, ('``', 'but'): 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "sentence = 'However, Lyon then ended talks with the Champions League runners-up and released a statement saying they were \"overjoyed that they can count on the presence of their captain, who will be a significant part of their 2018-19 season.\"But agent Jean-Pierre Bernes said Fekir, on World Cup duty with France, could still end up at Anfield. \"Why hasn\\'t he signed? Because it is not finished. The story is not finished,\" Bernes said. Fekir came on for the final 20 minutes as France beat Australia in their World Cup opener.'\n",
    "ctr = Counter(bigrams(word_tokenize(sentence.lower())))\n",
    "print(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 5),\n",
       " ('.', 5),\n",
       " ('the', 4),\n",
       " ('on', 3),\n",
       " ('their', 3),\n",
       " ('``', 3),\n",
       " ('cup', 2),\n",
       " ('said', 2),\n",
       " ('finished', 2),\n",
       " ('is', 2)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(word_tokenize(sentence.lower()))\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEoCAYAAAC3oe14AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZhzvcLHzSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4dd4d7fc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fdist.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'champions': 1, 'final': 1, 'presence': 1, 'story': 1})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "cfdist = ConditionalFreqDist(bigrams(word_tokenize(sentence.lower())))\n",
    "print(cfdist['the']['champions'])\n",
    "cfdist['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in [('their', 1)]\n",
      "for [('the', 1)]\n",
      "cup [('duty', 1)]\n",
      "captain [(',', 1)]\n",
      "on [('the', 1)]\n",
      "that [('they', 1)]\n",
      "jean-pierre [('bernes', 1)]\n",
      "australia [('in', 1)]\n",
      "saying [('they', 1)]\n",
      "can [('count', 1)]\n",
      "still [('end', 1)]\n",
      "said [('fekir', 1)]\n",
      "duty [('with', 1)]\n",
      "20 [('minutes', 1)]\n",
      "? [('because', 1)]\n",
      "'' [('bernes', 1)]\n",
      "part [('of', 1)]\n",
      "finished [('.', 1)]\n",
      "season [('.', 1)]\n",
      "and [('released', 1)]\n",
      "is [('not', 2)]\n",
      "their [('world', 1)]\n",
      "presence [('of', 1)]\n",
      "came [('on', 1)]\n",
      "he [('signed', 1)]\n",
      "overjoyed [('that', 1)]\n",
      "world [('cup', 2)]\n",
      "be [('a', 1)]\n",
      "minutes [('as', 1)]\n",
      "2018-19 [('season', 1)]\n",
      "opener [('.', 1)]\n",
      "bernes [('said', 2)]\n",
      "has [(\"n't\", 1)]\n",
      "at [('anfield', 1)]\n",
      "but [('agent', 1)]\n",
      "with [('the', 1)]\n",
      "significant [('part', 1)]\n",
      "of [('their', 2)]\n",
      "could [('still', 1)]\n",
      "anfield [('.', 1)]\n",
      "why [('has', 1)]\n",
      "as [('france', 1)]\n",
      "champions [('league', 1)]\n",
      "runners-up [('and', 1)]\n",
      "who [('will', 1)]\n",
      "agent [('jean-pierre', 1)]\n",
      "beat [('australia', 1)]\n",
      "ended [('talks', 1)]\n",
      "statement [('saying', 1)]\n",
      "fekir [(',', 1)]\n",
      "the [('story', 1)]\n",
      "final [('20', 1)]\n",
      ", [(\"''\", 1)]\n",
      "`` [('overjoyed', 1)]\n",
      "they [('were', 1)]\n",
      "will [('be', 1)]\n",
      "then [('ended', 1)]\n",
      "a [('significant', 1)]\n",
      "it [('is', 1)]\n",
      "released [('a', 1)]\n",
      "league [('runners-up', 1)]\n",
      "n't [('he', 1)]\n",
      "up [('at', 1)]\n",
      "however [(',', 1)]\n",
      "were [('``', 1)]\n",
      ". [('``', 2)]\n",
      "talks [('with', 1)]\n",
      "because [('it', 1)]\n",
      "not [('finished', 2)]\n",
      "end [('up', 1)]\n",
      "story [('is', 1)]\n",
      "lyon [('then', 1)]\n",
      "count [('on', 1)]\n",
      "signed [('?', 1)]\n",
      "france [('beat', 1)]\n"
     ]
    }
   ],
   "source": [
    "for k in cfdist.keys():\n",
    "    print(k, cfdist[k].most_common(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('However', 'RB')\n",
      "(',', ',')\n",
      "('Lyon', 'NNP')\n",
      "('then', 'RB')\n",
      "('ended', 'VBD')\n",
      "('talks', 'NNS')\n",
      "('with', 'IN')\n",
      "('the', 'DT')\n",
      "('Champions', 'NNP')\n",
      "('League', 'NNP')\n",
      "('runners-up', 'NN')\n",
      "('and', 'CC')\n",
      "('released', 'VBD')\n",
      "('a', 'DT')\n",
      "('statement', 'NN')\n",
      "('saying', 'VBG')\n",
      "('they', 'PRP')\n",
      "('were', 'VBD')\n",
      "('``', '``')\n",
      "('overjoyed', 'VBN')\n",
      "('that', 'IN')\n",
      "('they', 'PRP')\n",
      "('can', 'MD')\n",
      "('count', 'VB')\n",
      "('on', 'IN')\n",
      "('the', 'DT')\n",
      "('presence', 'NN')\n",
      "('of', 'IN')\n",
      "('their', 'PRP$')\n",
      "('captain', 'NN')\n",
      "(',', ',')\n",
      "('who', 'WP')\n",
      "('will', 'MD')\n",
      "('be', 'VB')\n",
      "('a', 'DT')\n",
      "('significant', 'JJ')\n",
      "('part', 'NN')\n",
      "('of', 'IN')\n",
      "('their', 'PRP$')\n",
      "('2018-19', 'JJ')\n",
      "('season', 'NN')\n",
      "('.', '.')\n",
      "(\"''\", \"''\")\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "sentence = 'However, Lyon then ended talks with the Champions League runners-up and \\\n",
    "            released a statement saying they were \"overjoyed that they can count on the presence of their captain, \\\n",
    "            who will be a significant part of their 2018-19 season.\"'\n",
    "for tup in pos_tag(word_tokenize(sentence)):\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Word Stemming\n",
    "Word stemming means removing affixes from words and return the root word. \n",
    "\n",
    "Ex: The stem of the word working => work.\n",
    "\n",
    "Search engines use this technique when indexing pages, so many people write different versions for the same word and all of them are stemmed to the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cacti\n",
      "gees\n",
      "work\n",
      "rock\n",
      "runner\n",
      "run\n",
      "increas\n",
      "increas\n",
      "increment\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "word_list = ['cats', 'cacti', 'geese', 'working', 'rocks', 'runners', 'running', 'increase', 'increasing', 'incremental']\n",
    "for word in word_list:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing Words Using WordNet\n",
    "Word lemmatizing is similar to stemming, but the difference is the result of lemmatizing is a real word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "working\n",
      "rock\n",
      "runner\n",
      "running\n",
      "increase\n",
      "increasing\n",
      "incremental\n",
      "good\n",
      "better\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in word_list:\n",
    "    print(lemmatizer.lemmatize(word))\n",
    "print(lemmatizer.lemmatize(\"better\", pos='a'))\n",
    "print(lemmatizer.lemmatize(\"better\", pos='v'))\n",
    "print(lemmatizer.lemmatize(\"better\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synonyms, Definitions, Usage and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a symptom of some physical hurt or disorder\n",
      "['the patient developed severe pain and distension']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets(\"pain\")\n",
    "print(syn[0].definition())\n",
    "print(syn[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system', 'calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "for syn in wordnet.synsets('Computer'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['large', 'big', 'big']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"small\"):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brown and Indian Corpora\n",
    "Include words and sentences, tagged as well as raw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n",
      "-----------------\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "-----------------\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      "-----------------\n",
      "[('The', 'DET'), ('Fulton', 'NOUN'), ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "print(brown.sents())\n",
    "print('-----------------')\n",
    "print(brown.words())\n",
    "print('-----------------')\n",
    "print(brown.tagged_words())\n",
    "print('-----------------')\n",
    "print(brown.tagged_words(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bangla.pos', 'hindi.pos', 'marathi.pos', 'telugu.pos']\n",
      "['पूर्ण', 'प्रतिबंध', 'हटाओ', ':', 'इराक', 'संयुक्त', ...]\n",
      "['జనం', 'ఎవరి', 'దారిన', 'వారు', 'వెళ్లిపోతున్నారు', '.']\n",
      "[(\"''\", 'SYM'), ('सनातनवाद्यांनी', 'NN'), ('व', 'CC'), ('प्रतिगाम्यांनी', 'NN'), ('समाज', 'NN'), ('रसातळाला', 'NN'), ('नेला', 'VM'), ('असताना', 'VAUX'), ('या', 'DEM'), ('अंधारात', 'NN'), ('बाळशास्त्री', 'NNPC'), ('जांभेकर', 'NNP'), ('यांनी', 'PRP'), (\"'दर्पण'च्या\", 'NNP'), ('माध्यमातून', 'NN'), ('पहिली', 'QO'), ('ज्ञानज्योत', 'NN'), ('तेववली', 'VM'), (',', 'SYM'), (\"''\", 'SYM'), ('असे', 'DEM'), ('प्रतिपादन', 'NN'), ('नटसम्राट', 'NNPC'), ('प्रभाकर', 'NNPC'), ('पणशीकर', 'NNP'), ('यांनी', 'PRP'), ('केले', 'VM'), ('.', 'SYM')]\n",
      "896\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import indian\n",
    "print( indian.fileids())\n",
    "print(indian.words('hindi.pos'))\n",
    "print(indian.sents('telugu.pos')[-1])\n",
    "print(indian.tagged_sents('marathi.pos')[0])\n",
    "print(len(indian.sents('bangla.pos')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For more info check out: \n",
    "http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "Another fast toolkit for NLP. Often better performance than NLTK but perhaps not as diverse.\n",
    "\n",
    "https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "(Do this later during lab sessions)\n",
    "\n",
    "https://spacy.io/usage/#installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1450\" height=\"417.0\" style=\"max-width: none; height: 417.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"190\">new</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"190\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"330\">Iphone</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"330\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"470\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"470\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">cheaper</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">than</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1030\">older,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1030\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1170\">degraded</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1170\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">Samsung.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,282.0 C70,142.0 320.0,142.0 320.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,284.0 L62,272.0 78,272.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M210,282.0 C210,212.0 315.0,212.0 315.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M210,284.0 L202,272.0 218,272.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M350,282.0 C350,212.0 455.0,212.0 455.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M350,284.0 L342,272.0 358,272.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M490,282.0 C490,212.0 595.0,212.0 595.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595.0,284.0 L603.0,272.0 587.0,272.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M630,282.0 C630,212.0 735.0,212.0 735.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,284.0 L743.0,272.0 727.0,272.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M910,282.0 C910,72.0 1305.0,72.0 1305.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910,284.0 L902,272.0 918,272.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M1050,282.0 C1050,142.0 1300.0,142.0 1300.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1050,284.0 L1042,272.0 1058,272.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M1190,282.0 C1190,212.0 1295.0,212.0 1295.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1190,284.0 L1182,272.0 1198,272.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M770,282.0 C770,2.0 1310.0,2.0 1310.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1310.0,284.0 L1318.0,272.0 1302.0,272.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "sentence = 'The new Iphone is cheaper than the older, degraded Samsung.'\n",
    "doc = nlp(sentence)\n",
    "opts = {'distance': 140}\n",
    "spacy.displacy.render(doc, jupyter=True, style='dep', options=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry compound []\n",
      "Kane nsubj [Harry]\n",
      "scored ROOT [Kane, goals, on, .]\n",
      "the det []\n",
      "two nummod []\n",
      "goals dobj [the, two, against, in]\n",
      "against prep [Tunasia]\n",
      "Tunasia pobj []\n",
      "in prep [Cup]\n",
      "FIFA compound []\n",
      "World compound []\n",
      "Cup pobj [FIFA, World, 2018]\n",
      "2018 nummod []\n",
      "on prep [June]\n",
      "18th amod []\n",
      "June pobj [18th]\n",
      ". punct []\n"
     ]
    }
   ],
   "source": [
    "for word in doc:\n",
    "    print(word, word.dep_, list(word.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DT DET\n",
      "new JJ ADJ\n",
      "Iphone NNP PROPN\n",
      "is VBZ VERB\n",
      "cheaper JJR ADJ\n",
      "than IN ADP\n",
      "the DT DET\n",
      "older JJR ADJ\n",
      ", , PUNCT\n",
      "degraded VBN VERB\n",
      "Samsung NNP PROPN\n",
      ". . PUNCT\n"
     ]
    }
   ],
   "source": [
    "for word in doc:\n",
    "    print(word, word.tag_, word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Kane PERSON\n",
      "two CARDINAL\n",
      "Tunasia GPE\n",
      "FIFA World Cup EVENT\n",
      "2018 DATE\n",
      "18th June DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Harry Kane scored the two goals against Tunasia in FIFA World Cup 2018 on 18th June.')\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember, Google is your best buddy :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
